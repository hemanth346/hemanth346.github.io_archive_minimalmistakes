---
title  : PCA
date : 2019-06-21
---

As discussed in my previous post, PCA is a dimensional reduction technique which uses Feature extraction.

## Geomtric Intuition

Before we understand how PCA works, we will first refresh the basics of visualizing data.

1. Each column/feature is a dimension(axis) on its own. 

2. Each row/datapoint is a vector

3. Each value in a row of dataframe is the corresponding feature's axis coordinate for given datapoint 


For Wisconsin(Diagnostic) Breast Cancer [dataset][Wisconsin data], we have total 32 attributes of which we take 30 to create a model. So there are 30 dimensions(axis) in its geometric space. 


Lets take an example of MNIST [dataset][MNIST data]. We have 28x28 pixel images, which are then stacked to row vectors of 784 columns, which effectively gives us 784 dimensions(axis) in a geometric space. Each datapoint or image gives us coordinates for these 784 dimensions. Once a model is finalised, we can classify a new datapoint based where it goes in this space *(i.e if we can visualize it.!, more on that later)*

When we do PCA - we are creating new features, i.e new dimensions to represent this data in geometric space. These new dimensions/features are called components.

New dimensions should be created such that we have maximum information retained using minimum no. of dimensions. Since we are selecting only top components with max info gain, we call this method as Principal Component Analysis(PCA)

And it can be proved that maximum information is retained on the dimension with maximum variance or minimum projection distance on its axis. Incidentally both maximum variance and minimum projection distance occurs on the same axis, as seen below

![Max variance_Source: (https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)] [first_component_gif]

> Note: It is observed that reducing the dimensionality of the data can lead to decreasing the variance of any statistical model, which might compensate the increase of bias (bias-variance trade-off). PCA also takes care of this as variance loss is monitored and minimized


So the question now is how to find these new features/components

## Finding Principal Components


#### 1. Standardize the data (Get input features to same variance)

For PCA, first we have to standardize the data. 

We have seen that preserving variance is key, to be able to do that we have to ensure all the input features are with same variance. 

	"More specifically, the reason why it is important to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem. 
	Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable." []

	Once the standardization is done, all the variables will be transformed to the same scale - with mean of 0 and Standard Deviation of 1(Variance is just square root of standard deviation and will be 1 as well)

#### 2. Finding new axis/dimension

As discussed above, the new dimension should be able to retain maximum information and it is observed at the dimension wih maximum variance.

Mathematically, it is obtained by finding **eigen vectors of Covariance matrix of given data.**  For detailed info read [making-sense-of-principal-component-analysis-eigenvectors-eigenvalues][making sense of pca] and [A Beginner's Guide to Eigenvectors, Eigenvalues, PCA, Covariance and Entropy][detailed pca]


So to get the new dimension with max info, we have to 

1. Calculate Covariance Matrix of given data

2. Find eigen values and eigen vectors

We won't be focusing on how to do that in this post, above posts do a very excellent job at that. We try to understand the significance. 

There are few good things with eigen values

1. The eigen vectors gives us the direction of the new dimension, and the eigen values provides information on how much variance has been preserved. 
	
	- We are not only finding new dimension with maximum, but able to quantify how much variance is preserved

2. We get the eigen vectors, such a way that there are in the direction with decreasing variance. 
	
	- i.e first eigen vector will be in direction with max. variance, 2nd in direction with 2nd max variance and so on, with last eigen vector in direction of the least variance preserved. 

	- That also means corresponding eigen values will be in decreasing order, with 1st having high values and last having least value.

3. We can find the percentage of variance is preserved in any no. of components/dimensions. We have to divide the sum of eigen values of these components to total sum of the eigen values


#### 3. Get new feature vectors

Once we got the new dimensions, we then have to project the data on the new dimensions to get thier corresponding points in the new feature space. Thereby effectively reducing the dimensionality of the data. 

To achieve that we just have to multiply data point with eigen vectors to get new features for datapoint






[1]: https://hemanth346.github.io/
[2]: https://brilliant.org/wiki/principal-component-analysis/
[3]: https://courses.cs.ut.ee/MTAT.03.227/2016_spring/uploads/Main/lecture-notes-9.pdf ""
[first_component_gif]: ../assets/images/first_component_gif.gif "maximum variance and minimum projection distance occurs on the same axis"

[Wisconsin data]: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
[MNIST data]: http://yann.lecun.com/exdb/mnist/
[detailed pca]: https://skymind.ai/wiki/eigenvector
[making sense of pca]: https://stats.stackexchange.com/a/140579

amoeba (https://stats.stackexchange.com/users/28666/amoeba), Making sense of principal component analysis, eigenvectors & eigenvalues, URL (version: 2018-03-14): https://stats.stackexchange.com/q/140579

http://colah.github.io/posts/2014-10-Visualizing-MNIST/